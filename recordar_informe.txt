JUSTIFICACION DEL MODELO:
- la LSTM de pytorch devuelve por defecto los hidden states concatenados (forward y backward pq es bidireccional)
- Vamos a hacer una media de los hidden states porque estos contienen info de toda la frase y lo que queremos es
clasificar la frase entera en SA

- Si devolvemos ouputs, sentiments tendremos 2 errores/accuracy => bueno: podemos analizar que parte de las dos hace mejor el MODELO
malo: cuenta como dos modelos independientes o como uno? 
necesitaremos 2 errores (CrossEntropyLoss) (TRAIN.PY)

- Linear: 
    # in_features = hidden_size si media, 2*hidden_size si concatenaciÃ³n
    # out_features = 3 pq tenemos 3 sentimientos (positivo, negativo y neutro)

IDEA: si va mal una solo lineal en clasificacion, hacer una MLP
IDEA: Agrega dropout entre LSTM y capas lineales.

COMENTARIOS SOBRE LOS MODELOS:
- el de 2 capas va un poco mejor que el de 1
- de los de dos capas, parece que el de hidden_size 256 va mejor que el de 128 pero hay que ver cuantas
aciertan de cada etiqueta de ner
- como vemos en la grafica de la perdida de ner en el modelo de una capa, hay cambios bruscos en la
perdida => vamos a incluir gradient clipping

ahora mery va a probar con LrPlateau + gradient clipping y yo voy a probar con gradient